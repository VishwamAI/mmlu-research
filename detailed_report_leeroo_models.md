# Detailed Report on Leeroo Models for MMLU Project

## Abstract
In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral’s accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator nearly matches GPT4’s performance at half the cost and even exceeds GPT4’s results with a 25% cost reduction. These findings illustrate the potential of our architecture in creating state-of-the-art and cost-effective LLMs by optimizing the synergy between multiple LLMs to achieve superior performance outcomes.

## Introduction
Developing foundational models is capital-intensive, necessitating vast computational resources and extensive high-quality data. Furthermore, the field is nearing the upper bounds of network size and data capacity, resulting in progressively marginal enhancements over existing models. This scenario echoes a critical juncture in human advancement, where the 'divide and conquer' strategy proposed by the authors aims to continue advancing the field efficiently.

## Architecture
Given a set of N expert models {e1, e2, ..., eN} and a sequence of m queries {q1, q2, ..., qm}, the Leeroo Orchestrator’s task is to optimally assign an expert model ei to each query qj. This assignment is governed by a policy function π, where π(qj) = ei, selecting the most appropriate expert for each query. Each expert’s response to a query is evaluated using a function eval(ei(qj)), yielding a score between 0 and 1 that reflects the response’s effectiveness. Additionally, there is an associated cost for each response, cost(ei(qj)), encompassing various factors such as computational resources and processing speed. The goal is to maximize the total evaluation scores while adhering to a budget constraint B.

## Synthetic Data Generation via Self-Play
Drawing inspiration from self-play in reinforcement learning and the proven success of synthetic data in various AI domains, we propose a generate-orchestrate-evaluate loop to create effective training data for the Leeroo-orch. The process begins with the generation of a question by a specialized generator. This question is then processed by the orchestrator, which selects the most suitable expert model for response. The Universe Construction algorithm aims to find a set of expert models that maximizes performance under a given budget constraint.

## Related Work
The landscape of benchmarks for assessing Large Language Models (LLMs) has grown significantly in recent years. The Massive Multitask Language Understanding (MMLU) benchmark, with its multiple-choice questions across 57 diverse domains, is particularly notable for its breadth. The Leeroo orchestrator's architecture leverages a policy network to dynamically select the most suitable expert model for each query, optimizing for cost, speed, and accuracy. The self-play loop for synthetic data generation and the Universe Construction algorithm are key innovations that enable the orchestrator to continuously improve and adapt by integrating new expert models.

## Results and Discussion
### Experiment Setting
Evaluation Setting. The evaluation of both baselines and Leeroo-orch models is conducted using the MMLU benchmark, which comprises multiple-choice questions across 57 diverse domains, such as mathematics, law, computer science, biology, and US history. To be compatible with OpenLLM Leaderboard, we use Eleuther AI Harness to evaluate our models. It calculates the likelihood of each choice in the question, and selects the answer with maximum likelihood, then 'accuracy' is used as the evaluation metric. The overall performance is calculated as the average accuracy of the model in 57 domains.

### Baselines
Our comparison includes a range of both open-source and closed-source LLMs. These comprise LLaMa 2 models with 7B, 13B, and 70B parameters, Mistral 7B, Mixtral 8x7B (employing token-level MoE), and the GPT3.5 and GPT4 models.

## Conclusion and Future Work
The ongoing development of our orchestrator, fueled by the self-play loop, promises substantial improvements in upcoming iterations. When choosing the best open-source model for each question in the MMLU benchmark, we would reach an accuracy of 98% at the computational cost of approximately a 13 billion parameter model. This indicates substantial room for growth and optimization. The seamless integration of new expert models as they emerge are central to our system’s design. This continuous process of assessment and adaptation not only enhances the Leeroo-orch’s versatility but also its capacity to harness emerging AI advancements. Our vision revolves around a fundamental principle: by narrowing the focus of language models, we unlock new horizons of effectiveness and efficiency in the realm of LLMs.
